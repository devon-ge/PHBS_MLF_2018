{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "\n",
    "import os, time, shutil, argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, rgb2gray, lab2rgb\n",
    "from skimage import io\n",
    "\n",
    "from functools import partial\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColorizationNet(nn.Module):\n",
    "    def __init__(self, midlevel_input_size=128, global_input_size=512):\n",
    "        super(ColorizationNet, self).__init__()\n",
    "        # Fusion layer to combine midlevel and global features\n",
    "        self.midlevel_input_size = midlevel_input_size\n",
    "        self.global_input_size = global_input_size\n",
    "        self.fusion = nn.Linear(midlevel_input_size + global_input_size, midlevel_input_size)\n",
    "        self.bn1 = nn.BatchNorm1d(midlevel_input_size)\n",
    "\n",
    "        # Convolutional layers and upsampling\n",
    "        self.deconv1_new = nn.ConvTranspose2d(midlevel_input_size, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.conv1 = nn.Conv2d(midlevel_input_size, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.conv2 = nn.Conv2d(128, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(64)\n",
    "        self.conv4 = nn.Conv2d(64, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.bn5 = nn.BatchNorm2d(32)\n",
    "        self.conv5 = nn.Conv2d(32, 2, kernel_size=3, stride=1, padding=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2)\n",
    "\n",
    "        print('Loaded colorization net.')\n",
    "\n",
    "    def forward(self, midlevel_input): #, global_input):\n",
    "        \n",
    "        # Convolutional layers and upsampling\n",
    "        x = F.relu(self.bn2(self.conv1(midlevel_input)))\n",
    "        x = self.upsample(x)\n",
    "        x = F.relu(self.bn3(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.upsample(x)\n",
    "        x = F.sigmoid(self.conv4(x))\n",
    "        x = self.upsample(self.conv5(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class ColorNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ColorNet, self).__init__()\n",
    "        \n",
    "        # Build ResNet and change first conv layer to accept single-channel input\n",
    "        resnet_gray_model = models.resnet18(num_classes=365)\n",
    "        resnet_gray_model.conv1.weight = nn.Parameter(resnet_gray_model.conv1.weight.sum(dim=1).unsqueeze(1).data)\n",
    "        \n",
    "        # Only needed if not resuming from a checkpoint: load pretrained ResNet-gray model\n",
    "        if torch.cuda.is_available(): # and only if gpu is available\n",
    "            resnet_gray_weights = torch.load('pretrained/resnet_gray_weights.pth.tar') #torch.load('pretrained/resnet_gray.tar')['state_dict']\n",
    "            resnet_gray_model.load_state_dict(resnet_gray_weights)\n",
    "            print('Pretrained ResNet-gray weights loaded')\n",
    "\n",
    "        # Extract midlevel and global features from ResNet-gray\n",
    "        self.midlevel_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:6])\n",
    "        self.global_resnet = nn.Sequential(*list(resnet_gray_model.children())[0:9])\n",
    "        self.fusion_and_colorization_net = ColorizationNet()\n",
    "\n",
    "    def forward(self, input_image):\n",
    "\n",
    "        # Pass input through ResNet-gray to extract features\n",
    "        midlevel_output = self.midlevel_resnet(input_image)\n",
    "        # global_output = self.global_resnet(input_image)\n",
    "\n",
    "        # Combine features in fusion layer and upsample\n",
    "        output = self.fusion_and_colorization_net(midlevel_output) #, global_output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.switch_backend('agg')\n",
    "\n",
    "def save_checkpoint(state, is_best_so_far, filename='checkpoints/checkpoint.pth.tar'):\n",
    "    '''Saves checkpoint, and replace the old best model if the current model is better'''\n",
    "    torch.save(state, filename)\n",
    "    if is_best_so_far:\n",
    "        shutil.copyfile(filename, 'checkpoints/model_best.pth.tar')\n",
    "\n",
    "class AverageMeter(object):\n",
    "    '''An easy way to compute and store both average and current values'''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def visualize_image(grayscale_input, ab_input=None, show_image=False, save_path=None, save_name=None):\n",
    "    '''Show or save image given grayscale (and ab color) inputs. Input save_path in the form {'grayscale': '/path/', 'colorized': '/path/'}'''\n",
    "    plt.clf() # clear matplotlib plot\n",
    "    ab_input = ab_input.cpu()\n",
    "    grayscale_input = grayscale_input.cpu()    \n",
    "    if ab_input is None:\n",
    "        grayscale_input = grayscale_input.squeeze().numpy() \n",
    "        if save_path is not None and save_name is not None: \n",
    "            plt.imsave(grayscale_input, '{}.{}'.format(save_path['grayscale'], save_name) , cmap='gray')\n",
    "        if show_image: \n",
    "            plt.imshow(grayscale_input, cmap='gray')\n",
    "            plt.show()\n",
    "    else: \n",
    "        color_image = torch.cat((grayscale_input, ab_input), 0).numpy()\n",
    "        color_image = color_image.transpose((1, 2, 0))  \n",
    "        color_image[:, :, 0:1] = color_image[:, :, 0:1] * 100\n",
    "        color_image[:, :, 1:3] = color_image[:, :, 1:3] * 255 - 128   \n",
    "        color_image = lab2rgb(color_image.astype(np.float64))\n",
    "        grayscale_input = grayscale_input.squeeze().numpy()\n",
    "        if save_path is not None and save_name is not None:\n",
    "            plt.imsave(arr=grayscale_input, fname='{}{}'.format(save_path['grayscale'], save_name), cmap='gray')\n",
    "            plt.imsave(arr=color_image, fname='{}{}'.format(save_path['colorized'], save_name))\n",
    "        if show_image: \n",
    "            f, axarr = plt.subplots(1, 2)\n",
    "            axarr[0].imshow(grayscale_input, cmap='gray')\n",
    "            axarr[1].imshow(color_image)\n",
    "            plt.show()\n",
    "\n",
    "class GrayscaleImageFolder(datasets.ImageFolder):\n",
    "    '''Custom images folder, which converts images to grayscale before loading'''\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.imgs[index]\n",
    "        img = self.loader(path)\n",
    "        if self.transform is not None:\n",
    "            img_original = self.transform(img)\n",
    "            img_original = np.asarray(img_original)\n",
    "            img_lab = rgb2lab(img_original)\n",
    "            img_lab = (img_lab + 128) / 255\n",
    "            img_ab = img_lab[:, :, 1:3]\n",
    "            img_ab = torch.from_numpy(img_ab.transpose((2, 0, 1))).float()\n",
    "            img_original = rgb2gray(img_original)\n",
    "            img_original = torch.from_numpy(img_original).unsqueeze(0).float()\n",
    "        if self.target_transform is not None:\n",
    "            target = self.target_transform(target)\n",
    "        return img_original, img_ab, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    '''Train model on data in train_loader for a single epoch'''\n",
    "    print('Starting training epoch {}'.format(epoch))\n",
    "\n",
    "    # Prepare value counters and timers\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # Switch model to train mode\n",
    "    model.train()\n",
    "    \n",
    "    # Train for single eopch\n",
    "    end = time.time()\n",
    "    for i, (input_gray, input_ab, target) in enumerate(train_loader):\n",
    "        \n",
    "        # Use GPU if available\n",
    "        input_gray_variable = Variable(input_gray).cuda() if use_gpu else Variable(input_gray)\n",
    "        input_ab_variable = Variable(input_ab).cuda() if use_gpu else Variable(input_ab)\n",
    "        target_variable = Variable(target).cuda() if use_gpu else Variable(target)\n",
    "\n",
    "        # Record time to load data (above)\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Run forward pass\n",
    "        output_ab = model(input_gray_variable) # throw away class predictions\n",
    "        loss = criterion(output_ab, input_ab_variable) # MSE\n",
    "        \n",
    "        # Record loss and measure accuracy\n",
    "        losses.update(loss.data[0], input_gray.size(0))\n",
    "        \n",
    "        # Compute gradient and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record time to do forward and backward passes\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print model accuracy -- in the code below, val refers to value, not validation\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses)) \n",
    "\n",
    "    print('Finished training epoch {}'.format(epoch))\n",
    "\n",
    "def validate(val_loader, model, criterion, save_images, epoch):\n",
    "    '''Validate model on data in val_loader'''\n",
    "    print('Starting validation.')\n",
    "\n",
    "    # Prepare value counters and timers\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    \n",
    "    # Switch model to validation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Run through validation set\n",
    "    end = time.time()\n",
    "    for i, (input_gray, input_ab, target) in enumerate(val_loader):\n",
    "        \n",
    "        # Use GPU if available\n",
    "        target = target.cuda() if use_gpu else target\n",
    "        input_gray_variable = Variable(input_gray, volatile=True).cuda() if use_gpu else Variable(input_gray, volatile=True)\n",
    "        input_ab_variable = Variable(input_ab, volatile=True).cuda() if use_gpu else Variable(input_ab, volatile=True)\n",
    "        target_variable = Variable(target, volatile=True).cuda() if use_gpu else Variable(target, volatile=True)\n",
    "       \n",
    "        # Record time to load data (above)\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "        # Run forward pass\n",
    "        output_ab = model(input_gray_variable) # throw away class predictions\n",
    "        loss = criterion(output_ab, input_ab_variable) # check this!\n",
    "        \n",
    "        # Record loss and measure accuracy\n",
    "        losses.update(loss.data[0], input_gray.size(0))\n",
    "\n",
    "        # Save images to file\n",
    "        if save_images:\n",
    "            for j in range(len(output_ab)):\n",
    "                save_path = {'grayscale': os.path.join(args.data, 'outputs/gray/'), 'colorized': os.path.join(args.data, 'outputs/color/')}\n",
    "                save_name = 'img-{}-epoch-{}.jpg'.format(i * val_loader.batch_size + j, epoch)\n",
    "                visualize_image(input_gray[j], ab_input=output_ab[j].data, show_image=False, save_path=save_path, save_name=save_name)\n",
    "\n",
    "        # Record time to do forward passes and save images\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "        \n",
    "        # Print model accuracy -- in the code below, val refers to both value and validation\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Validate: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   i, len(val_loader), batch_time=batch_time, loss=losses))\n",
    "\n",
    "    print('Finished validation.')\n",
    "    return losses.avg\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_StoreAction(option_strings=['-f', '--fuck'], dest='fuck', nargs=None, const=None, default='', type=<class 'str'>, choices=None, help='fuck off the -f', metavar='N')"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse arguments and prepare program\n",
    "parser = argparse.ArgumentParser(description='Training and Using ColorNet')\n",
    "parser.add_argument('-data', default='', type=str, metavar='DIR', help='path to dataset')\n",
    "parser.add_argument('-j', '--workers', default=0, type=int, metavar='N', help='number of data loading workers (default: 0)')\n",
    "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to .pth file checkpoint (default: none)')\n",
    "parser.add_argument('--epochs', default=10, type=int, metavar='N', help='number of total epochs to run')\n",
    "parser.add_argument('--start-epoch', default=0, type=int, metavar='N', help='manual epoch number (overridden if loading from checkpoint)')\n",
    "parser.add_argument('-b', '--batch-size', default=16, type=int, metavar='N', help='size of mini-batch (default: 16)')\n",
    "parser.add_argument('--lr', '--learning-rate', default=0.1, type=float, metavar='LR', help='learning rate at start of training')\n",
    "parser.add_argument('--weight-decay', '--wd', default=1e-4, type=float, metavar='W', help='weight decay (default: 1e-4)')\n",
    "parser.add_argument('-e', '--evaluate', dest='evaluate', action='store_true', help='use this flag to validate without training')\n",
    "parser.add_argument('--print-freq', '-p', default=1, type=int, metavar='N', help='print frequency (default: 10)')\n",
    "parser.add_argument('-f', '--fuck', default='', type=str, metavar='N', help='fuck off the -f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Current best losses\n",
    "best_losses = 1000.0\n",
    "use_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arguments: Namespace(batch_size=16, data='./', epochs=10, evaluate=False, fuck='/Users/aaronwg/Library/Jupyter/runtime/kernel-41363dec-d660-413d-954b-d0545c4c4d2b.json', lr=0.1, print_freq=1, resume='', start_epoch=0, weight_decay=0.0001, workers=0)\n"
     ]
    }
   ],
   "source": [
    "global args, best_losses, use_gpu\n",
    "args = parser.parse_args()\n",
    "print('Arguments: {}'.format(args))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded colorization net.\n"
     ]
    }
   ],
   "source": [
    "# Create model  # models.resnet18(num_classes=365)\n",
    "model = ColorNet()\n",
    "    \n",
    "# Use GPU if available\n",
    "if use_gpu:\n",
    "    model.cuda()\n",
    "    print('Loaded model onto GPU.')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "# Create loss function, optimizer #criterion = nn.CrossEntropyLoss().cuda() if use_gpu else nn.CrossEntropyLoss()\n",
    "criterion = nn.MSELoss().cuda() if use_gpu else nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded training data.\n",
      "Loaded validation data.\n"
     ]
    }
   ],
   "source": [
    "# Load data from pre-defined (imagenet-style) structure\n",
    "if not args.evaluate:\n",
    "    train_directory = os.path.join(args.data, 'train')\n",
    "    train_transforms = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(256),\n",
    "    transforms.RandomHorizontalFlip()\n",
    "    ])\n",
    "    train_imagefolder = GrayscaleImageFolder(train_directory, train_transforms)\n",
    "    train_loader = torch.utils.data.DataLoader(train_imagefolder, batch_size=args.batch_size, shuffle=True, num_workers=args.workers)\n",
    "    print('Loaded training data.')\n",
    "val_transforms = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "])\n",
    "val_directory = os.path.join(args.data, 'val')\n",
    "val_imagefolder = GrayscaleImageFolder(val_directory , val_transforms)\n",
    "val_loader = torch.utils.data.DataLoader(val_imagefolder, batch_size=args.batch_size, shuffle=False, num_workers=args.workers)\n",
    "print('Loaded validation data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.216 (0.216)\tLoss 0.4959 (0.4959)\t\n",
      "Finished validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:82: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.4959)"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If in evaluation (validation) mode, do not train\n",
    "if args.evaluate:\n",
    "    save_images = True\n",
    "    epoch = 0\n",
    "    initial_losses = validate(val_loader, model, criterion, save_images, epoch)\n",
    "\n",
    "        # # Save checkpoint after evaluation if desired\n",
    "        # save_checkpoint({\n",
    "        #     'epoch': epoch,\n",
    "        #     'best_losses': initial_losses,\n",
    "        #     'state_dict': model.state_dict(),\n",
    "        #     'optimizer': optimizer.state_dict(),\n",
    "        # }, False, 'checkpoints/evaluate-checkpoint.pth.tar')\n",
    "        \n",
    "#        return  \n",
    "    \n",
    "# Otherwise, train for given number of epochs\n",
    "validate(val_loader, model, criterion, False, 0) # validate before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py:1006: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:30: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/2]\tTime 8.578 (8.578)\tData 0.424 (0.424)\tLoss 0.2348 (0.2348)\t\n",
      "Epoch: [0][1/2]\tTime 24.975 (16.777)\tData 0.366 (0.395)\tLoss 189.5527 (94.8937)\t\n",
      "Finished training epoch 0\n",
      "Starting validation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:70: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:71: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:72: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:82: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "/anaconda3/lib/python3.6/site-packages/skimage/color/colorconv.py:985: UserWarning: Color data out of range: Z < 0 in 65536 pixels\n",
      "  warn('Color data out of range: Z < 0 in %s pixels' % invalid[0].size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/1]\tTime 0.269 (0.269)\tLoss 15.0242 (15.0242)\t\n",
      "Finished validation.\n",
      "Starting training epoch 1\n",
      "Epoch: [1][0/2]\tTime 28.231 (28.231)\tData 0.347 (0.347)\tLoss 15.0969 (15.0969)\t\n",
      "Epoch: [1][1/2]\tTime 31.788 (30.010)\tData 0.404 (0.376)\tLoss 34.2928 (24.6949)\t\n",
      "Finished training epoch 1\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.260 (0.260)\tLoss 104.5425 (104.5425)\t\n",
      "Finished validation.\n",
      "Starting training epoch 2\n",
      "Epoch: [2][0/2]\tTime 29.388 (29.388)\tData 0.362 (0.362)\tLoss 103.8999 (103.8999)\t\n",
      "Epoch: [2][1/2]\tTime 29.240 (29.314)\tData 0.370 (0.366)\tLoss 86.3789 (95.1394)\t\n",
      "Finished training epoch 2\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.347 (0.347)\tLoss 28.4680 (28.4680)\t\n",
      "Finished validation.\n",
      "Starting training epoch 3\n",
      "Epoch: [3][0/2]\tTime 19.535 (19.535)\tData 0.406 (0.406)\tLoss 28.5799 (28.5799)\t\n",
      "Epoch: [3][1/2]\tTime 27.848 (23.692)\tData 0.393 (0.400)\tLoss 8.7702 (18.6750)\t\n",
      "Finished training epoch 3\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.273 (0.273)\tLoss 35.5792 (35.5792)\t\n",
      "Finished validation.\n",
      "Starting training epoch 4\n",
      "Epoch: [4][0/2]\tTime 21.910 (21.910)\tData 0.389 (0.389)\tLoss 35.6266 (35.6266)\t\n",
      "Epoch: [4][1/2]\tTime 24.447 (23.178)\tData 0.370 (0.379)\tLoss 43.7115 (39.6691)\t\n",
      "Finished training epoch 4\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.240 (0.240)\tLoss 26.3298 (26.3298)\t\n",
      "Finished validation.\n",
      "Starting training epoch 5\n",
      "Epoch: [5][0/2]\tTime 26.260 (26.260)\tData 0.348 (0.348)\tLoss 26.4186 (26.4186)\t\n",
      "Epoch: [5][1/2]\tTime 26.197 (26.229)\tData 0.362 (0.355)\tLoss 5.3537 (15.8861)\t\n",
      "Finished training epoch 5\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.237 (0.237)\tLoss 1.1400 (1.1400)\t\n",
      "Finished validation.\n",
      "Starting training epoch 6\n",
      "Epoch: [6][0/2]\tTime 26.648 (26.648)\tData 0.330 (0.330)\tLoss 1.0457 (1.0457)\t\n",
      "Epoch: [6][1/2]\tTime 28.819 (27.734)\tData 0.360 (0.345)\tLoss 13.0860 (7.0658)\t\n",
      "Finished training epoch 6\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.301 (0.301)\tLoss 23.1349 (23.1349)\t\n",
      "Finished validation.\n",
      "Starting training epoch 7\n",
      "Epoch: [7][0/2]\tTime 30.859 (30.859)\tData 0.543 (0.543)\tLoss 22.9696 (22.9696)\t\n",
      "Epoch: [7][1/2]\tTime 31.722 (31.290)\tData 0.519 (0.531)\tLoss 19.6757 (21.3226)\t\n",
      "Finished training epoch 7\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.338 (0.338)\tLoss 7.7630 (7.7630)\t\n",
      "Finished validation.\n",
      "Starting training epoch 8\n",
      "Epoch: [8][0/2]\tTime 28.114 (28.114)\tData 0.693 (0.693)\tLoss 7.7392 (7.7392)\t\n",
      "Epoch: [8][1/2]\tTime 27.812 (27.963)\tData 0.502 (0.598)\tLoss 0.3356 (4.0374)\t\n",
      "Finished training epoch 8\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.317 (0.317)\tLoss 3.2254 (3.2254)\t\n",
      "Finished validation.\n",
      "Starting training epoch 9\n",
      "Epoch: [9][0/2]\tTime 26.903 (26.903)\tData 0.556 (0.556)\tLoss 3.2277 (3.2277)\t\n",
      "Epoch: [9][1/2]\tTime 32.296 (29.599)\tData 0.482 (0.519)\tLoss 10.8561 (7.0419)\t\n",
      "Finished training epoch 9\n",
      "Starting validation.\n",
      "Validate: [0/1]\tTime 0.296 (0.296)\tLoss 11.3219 (11.3219)\t\n",
      "Finished validation.\n"
     ]
    }
   ],
   "source": [
    "if args.evaluate:\n",
    "    save_images = True\n",
    "    epoch = 0\n",
    "    initial_losses = validate(val_loader, model, criterion, save_images, epoch)\n",
    "\n",
    "    # # Save checkpoint after evaluation if desired\n",
    "    # save_checkpoint({\n",
    "    #     'epoch': epoch,\n",
    "    #     'best_losses': initial_losses,\n",
    "    #     'state_dict': model.state_dict(),\n",
    "    #     'optimizer': optimizer.state_dict(),\n",
    "    # }, False, 'checkpoints/evaluate-checkpoint.pth.tar')\n",
    "else:\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        \n",
    "        # Train for one epoch, then validate\n",
    "        train(train_loader, model, criterion, optimizer, epoch)\n",
    "        if epoch % 10 == 0:\n",
    "            save_images = True\n",
    "            losses = validate(val_loader, model, criterion, save_images, epoch)\n",
    "        \n",
    "        # Save checkpoint, and replace the old best model if the current model is better\n",
    "        is_best_so_far = losses < best_losses\n",
    "        best_losses = max(losses, best_losses)\n",
    "        if is_best_so_far:\n",
    "            save_checkpoint({\n",
    "                'epoch': epoch + 1,\n",
    "                'best_losses': best_losses,\n",
    "                'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, is_best_so_far, os.path.join(args.data, 'checkpoints/checkpoint-epoch-{}.pth.tar'.format(epoch)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
